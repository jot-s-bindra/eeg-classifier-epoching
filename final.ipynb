{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file cleaned_eeg_data_left2.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 10527 =      0.000 ...    82.242 secs\n",
      "Ready.\n",
      "Reading 0 ... 10527  =      0.000 ...    82.242 secs...\n",
      "Opening raw data file cleaned_eeg_data_right.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 16255 =      0.000 ...   126.992 secs\n",
      "Ready.\n",
      "Reading 0 ... 16255  =      0.000 ...   126.992 secs...\n",
      "Opening raw data file cleaned_eeg_data_up.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 10136 =      0.000 ...    79.188 secs\n",
      "Ready.\n",
      "Reading 0 ... 10136  =      0.000 ...    79.188 secs...\n",
      "Opening raw data file cleaned_eeg_data_down.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 18327 =      0.000 ...   143.180 secs\n",
      "Ready.\n",
      "Reading 0 ... 18327  =      0.000 ...   143.180 secs...\n",
      "Not setting metadata\n",
      "41 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 41 events and 257 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "63 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 63 events and 257 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "39 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 39 events and 257 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "71 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 71 events and 257 original time points ...\n",
      "0 bad epochs dropped\n",
      "Dropped 1 epoch: 5\n",
      "Dropped 1 epoch: 5\n",
      "Dropped 1 epoch: 5\n",
      "Dropped 1 epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313062/803537539.py:7: RuntimeWarning: This filename (cleaned_eeg_data_left2.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  left_data = mne.io.read_raw_fif('cleaned_eeg_data_left2.fif', preload=True)\n",
      "/tmp/ipykernel_313062/803537539.py:8: RuntimeWarning: This filename (cleaned_eeg_data_right.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  right_data = mne.io.read_raw_fif('cleaned_eeg_data_right.fif', preload=True)\n",
      "/tmp/ipykernel_313062/803537539.py:9: RuntimeWarning: This filename (cleaned_eeg_data_up.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  up_data = mne.io.read_raw_fif('cleaned_eeg_data_up.fif', preload=True)\n",
      "/tmp/ipykernel_313062/803537539.py:10: RuntimeWarning: This filename (cleaned_eeg_data_down.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  down_data = mne.io.read_raw_fif('cleaned_eeg_data_down.fif', preload=True)\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "left_data = mne.io.read_raw_fif('cleaned_eeg_data_left2.fif', preload=True)\n",
    "right_data = mne.io.read_raw_fif('cleaned_eeg_data_right.fif', preload=True)\n",
    "up_data = mne.io.read_raw_fif('cleaned_eeg_data_up.fif', preload=True)\n",
    "down_data = mne.io.read_raw_fif('cleaned_eeg_data_down.fif', preload=True)\n",
    "\n",
    "epoch_duration = 2\n",
    "overlap = 0.5  \n",
    "left_events = mne.make_fixed_length_events(left_data, duration=epoch_duration)\n",
    "right_events = mne.make_fixed_length_events(right_data, duration=epoch_duration)\n",
    "up_events = mne.make_fixed_length_events(up_data, duration=epoch_duration)\n",
    "down_events = mne.make_fixed_length_events(down_data, duration=epoch_duration)\n",
    "\n",
    "left_epochs = mne.Epochs(left_data, events=left_events, tmin=0, tmax=epoch_duration, baseline=None, preload=True)\n",
    "right_epochs = mne.Epochs(right_data, events=right_events, tmin=0, tmax=epoch_duration, baseline=None, preload=True)\n",
    "up_epochs = mne.Epochs(up_data, events=up_events, tmin=0, tmax=epoch_duration, baseline=None, preload=True)\n",
    "down_epochs = mne.Epochs(down_data, events=down_events, tmin=0, tmax=epoch_duration, baseline=None, preload=True)\n",
    "\n",
    "left_epoch_test = left_epochs[5]\n",
    "right_epoch_test = right_epochs[5]\n",
    "up_epoch_test = up_epochs[5]\n",
    "down_epoch_test = down_epochs[5]\n",
    "\n",
    "left_epochs = left_epochs.drop(5)\n",
    "right_epochs = right_epochs.drop(5)\n",
    "up_epochs = up_epochs.drop(5)\n",
    "down_epochs = down_epochs.drop(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313062/3149584071.py:20: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  left_preprocessed = preprocess_epochs(left_epochs.get_data())\n",
      "/tmp/ipykernel_313062/3149584071.py:21: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  right_preprocessed = preprocess_epochs(right_epochs.get_data())\n",
      "/tmp/ipykernel_313062/3149584071.py:22: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  up_preprocessed = preprocess_epochs(up_epochs.get_data())\n",
      "/tmp/ipykernel_313062/3149584071.py:23: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  down_preprocessed = preprocess_epochs(down_epochs.get_data())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def preprocess_epochs(epochs_data, window_length=11, poly_order=2):\n",
    "    preprocessed_epochs = []\n",
    "\n",
    "    for epoch_data in epochs_data:\n",
    "        processed_data = []\n",
    "        for channel in range(epoch_data.shape[1]):\n",
    "            smoothed_data = savgol_filter(epoch_data[:, channel], window_length, poly_order)\n",
    "            processed_data.append(smoothed_data)\n",
    "\n",
    "        big_channel = np.concatenate(processed_data)\n",
    "\n",
    "        preprocessed_epochs.append(big_channel)\n",
    "\n",
    "    return preprocessed_epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "left_preprocessed = preprocess_epochs(left_epochs.get_data())\n",
    "right_preprocessed = preprocess_epochs(right_epochs.get_data())\n",
    "up_preprocessed = preprocess_epochs(up_epochs.get_data())\n",
    "down_preprocessed = preprocess_epochs(down_epochs.get_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 30.95%\n",
      "\n",
      "Classification Report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         6\n",
      "         1.0       0.18      0.17      0.17        12\n",
      "         2.0       0.20      0.14      0.17         7\n",
      "         3.0       0.43      0.59      0.50        17\n",
      "\n",
      "    accuracy                           0.31        42\n",
      "   macro avg       0.20      0.22      0.21        42\n",
      "weighted avg       0.26      0.31      0.28        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = np.vstack((left_preprocessed, right_preprocessed, up_preprocessed, down_preprocessed))\n",
    "labels = np.concatenate((np.zeros(len(left_preprocessed)),\n",
    "                         np.ones(len(right_preprocessed)),\n",
    "                         2 * np.ones(len(up_preprocessed)),\n",
    "                         3 * np.ones(len(down_preprocessed))))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "predicted_labels = lda.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "report = classification_report(y_test, predicted_labels)\n",
    "print(\"\\nClassification Report on test set:\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bands = {'Delta': (0.5, 4),\n",
    "              'Theta': (4, 8),\n",
    "              'Alpha': (8, 13),\n",
    "              'Beta': (13, 30),#should be more prevalent during our game\n",
    "              'Gamma': (30, 40)}  \n",
    "\n",
    "def compute_avg_band_amplitudes(epochs):\n",
    "    avg_band_amplitudes = []\n",
    "    for epoch in epochs:\n",
    "        channel_band_amplitudes = []\n",
    "        for channel_data in epoch:\n",
    "            for band in freq_bands.values():\n",
    "                fmin, fmax = band\n",
    "                sp = np.fft.fft(channel_data)\n",
    "                freq = np.fft.fftfreq(len(channel_data), d=1/left_data.info['sfreq'])\n",
    "                freq = freq[1:int(np.ceil(len(channel_data) / 4))]  \n",
    "                sp = sp[1:int(np.ceil(len(channel_data) / 4))]\n",
    "                sp = np.sqrt(sp.real**2 + sp.imag**2)\n",
    "                band_indices = np.logical_and(freq >= fmin, freq <= fmax)\n",
    "                band_amplitude = np.mean(sp[band_indices])\n",
    "                channel_band_amplitudes.append(band_amplitude)\n",
    "        avg_band_amplitudes.append(np.mean(channel_band_amplitudes))\n",
    "    return avg_band_amplitudes\n",
    "\n",
    "left_avg_band_amplitudes = compute_avg_band_amplitudes(left_epochs)\n",
    "right_avg_band_amplitudes = compute_avg_band_amplitudes(right_epochs)\n",
    "up_avg_band_amplitudes = compute_avg_band_amplitudes(up_epochs)\n",
    "down_avg_band_amplitudes = compute_avg_band_amplitudes(down_epochs)\n",
    "all_avg_band_amplitudes = left_avg_band_amplitudes + up_avg_band_amplitudes+down_avg_band_amplitudes+right_avg_band_amplitudes\n",
    "\n",
    "X2 = np.array(all_avg_band_amplitudes)\n",
    "labels2 = np.concatenate((\n",
    "    np.zeros(len(left_avg_band_amplitudes)),        # Left category labeled as 0\n",
    "    np.ones(len(right_avg_band_amplitudes)),       # Right category labeled as 1\n",
    "    np.full(len(up_avg_band_amplitudes), 2),       # Up category labeled as 2\n",
    "    np.full(len(down_avg_band_amplitudes), 3)      # Down category labeled as 3\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set for second model: 47.62%\n",
      "\n",
      "Classification Report on test set for second model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.33      0.36         6\n",
      "         1.0       0.50      0.42      0.45        12\n",
      "         2.0       1.00      0.00      0.00         7\n",
      "         3.0       0.48      0.76      0.59        17\n",
      "\n",
      "    accuracy                           0.48        42\n",
      "   macro avg       0.60      0.38      0.35        42\n",
      "weighted avg       0.56      0.48      0.42        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, labels2, test_size=0.2, random_state=42)\n",
    "X_train2 = X_train2.reshape(X_train2.shape[0], -1)\n",
    "X_test2 = X_test2.reshape(X_test2.shape[0], -1)\n",
    "\n",
    "lda2 = LinearDiscriminantAnalysis()\n",
    "lda2.fit(X_train2, y_train2)\n",
    "\n",
    "predicted_labels2 = lda2.predict(X_test2)\n",
    "\n",
    "accuracy2 = accuracy_score(y_test2, predicted_labels2)\n",
    "print(f\"Accuracy on test set for second model: {accuracy2 * 100:.2f}%\")\n",
    "\n",
    "report2 = classification_report(y_test2, predicted_labels2, zero_division=1) \n",
    "print(\"\\nClassification Report on test set for second model:\")\n",
    "print(report2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313062/259963410.py:24: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  processedRaw_left = preprocessRaw_epochs(left_epochs.get_data())\n",
      "/tmp/ipykernel_313062/259963410.py:25: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  processedRaw_right = preprocessRaw_epochs(right_epochs.get_data())\n",
      "/tmp/ipykernel_313062/259963410.py:26: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  processedRaw_up = preprocessRaw_epochs(up_epochs.get_data())\n",
      "/tmp/ipykernel_313062/259963410.py:27: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  processedRaw_down = preprocessRaw_epochs(down_epochs.get_data())\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocessRaw_epochs(epochs_data, window_length=11, poly_order=2):\n",
    "    processedRaw = []\n",
    "    min_length = min(len(epoch[0]) for epoch in epochs_data)\n",
    "\n",
    "    for epoch_data in epochs_data:\n",
    "        processed_data = []\n",
    "        for channel_data in epoch_data:\n",
    "            smoothed_data = savgol_filter(channel_data, window_length, poly_order)\n",
    "            processed_data.append(smoothed_data[:min_length])\n",
    "\n",
    "        processedRaw.append(np.array(processed_data))\n",
    "\n",
    "    return processedRaw\n",
    "\n",
    "\n",
    "\n",
    "processedRaw_left = preprocessRaw_epochs(left_epochs.get_data())\n",
    "processedRaw_right = preprocessRaw_epochs(right_epochs.get_data())\n",
    "processedRaw_up = preprocessRaw_epochs(up_epochs.get_data())\n",
    "processedRaw_down = preprocessRaw_epochs(down_epochs.get_data())\n",
    "\n",
    "\n",
    "X_processedRaw = np.vstack((processedRaw_left, processedRaw_right, processedRaw_up, processedRaw_down))\n",
    "labels_processedRaw = np.concatenate((np.zeros(len(processedRaw_left)),\n",
    "                                      np.ones(len(processedRaw_right)),\n",
    "                                      2 * np.ones(len(processedRaw_up)),\n",
    "                                      3 * np.ones(len(processedRaw_down))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on processedRaw test set: 54.76%\n",
      "\n",
      "Classification Report on processedRaw test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.17      0.22         6\n",
      "         1.0       0.38      0.25      0.30        12\n",
      "         2.0       0.44      0.57      0.50         7\n",
      "         3.0       0.68      0.88      0.77        17\n",
      "\n",
      "    accuracy                           0.55        42\n",
      "   macro avg       0.46      0.47      0.45        42\n",
      "weighted avg       0.50      0.55      0.51        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_processedRaw_flat = np.array([epoch.flatten() for epoch in X_processedRaw])\n",
    "\n",
    "X_train_processedRaw_flat, X_test_processedRaw_flat, y_train_processedRaw, y_test_processedRaw = train_test_split(\n",
    "    X_processedRaw_flat, labels_processedRaw, test_size=0.2, random_state=42)\n",
    "\n",
    "lda3 = LinearDiscriminantAnalysis()\n",
    "lda3.fit(X_train_processedRaw_flat, y_train_processedRaw)\n",
    "\n",
    "predicted_labels_processedRaw = lda3.predict(X_test_processedRaw_flat)\n",
    "\n",
    "accuracy_processedRaw = accuracy_score(y_test_processedRaw, predicted_labels_processedRaw)\n",
    "print(f\"Accuracy on processedRaw test set: {accuracy_processedRaw * 100:.2f}%\")\n",
    "\n",
    "report_processedRaw = classification_report(y_test_processedRaw, predicted_labels_processedRaw)\n",
    "print(\"\\nClassification Report on processedRaw test set:\")\n",
    "print(report_processedRaw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Model: Predicted Label - 1.0, Max Probability - 0.74741166\n",
      "2nd Model: Max Class - 3, Max Probability - 0.4037\n",
      "3rd Model: Predicted Class - 1.0, Max Probability - 0.80053243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313062/2118879545.py:4: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  up_test_preprocessed = preprocess_epochs(right_epoch_test.get_data())\n",
      "/tmp/ipykernel_313062/2118879545.py:20: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  processedRaw_up_epoch_test = preprocessRaw_epochs(up_epoch_test.get_data())\n"
     ]
    }
   ],
   "source": [
    "#take eeg data\n",
    "#apply ica\n",
    "#get epochs(taking up_epoch_test)\n",
    "up_test_preprocessed = preprocess_epochs(right_epoch_test.get_data())\n",
    "up_epoch_test_np = np.array(up_test_preprocessed)\n",
    "predicted_labels1 = lda.predict(up_epoch_test_np)\n",
    "predicted_probabilities1 = lda.predict_proba(up_epoch_test_np)\n",
    "max_probabilities1 = np.max(predicted_probabilities1, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "up_test_avg_band_amplitudes = compute_avg_band_amplitudes(up_epoch_test)\n",
    "up_test_avg_band_amplitudes_np = np.array(up_test_avg_band_amplitudes)\n",
    "up_test_avg_amplitude_reshaped = up_test_avg_band_amplitudes_np.reshape(up_test_avg_band_amplitudes_np.shape[0], -1)\n",
    "predicted_probabilities_up_avg_amplitude = lda2.predict_proba(up_test_avg_amplitude_reshaped)\n",
    "max_probabilities_up_avg_amplitude = np.max(predicted_probabilities_up_avg_amplitude, axis=1)\n",
    "max_class_indices_up_avg_amplitude = np.argmax(predicted_probabilities_up_avg_amplitude, axis=1)\n",
    "\n",
    "\n",
    "processedRaw_up_epoch_test = preprocessRaw_epochs(up_epoch_test.get_data())\n",
    "\n",
    "X_processedRaw_up_epoch_test = np.array([epoch.flatten() for epoch in processedRaw_up_epoch_test])\n",
    "\n",
    "predicted_labels_up_epoch_test = lda3.predict(X_processedRaw_up_epoch_test)\n",
    "predicted_probabilities_up_epoch_test = lda3.predict_proba(X_processedRaw_up_epoch_test)\n",
    "\n",
    "print(f\"1st Model: Predicted Label - {predicted_labels1[0]}, Max Probability - {max_probabilities1[0]:.8f}\")\n",
    "print(f\"2nd Model: Max Class - {max_class_indices_up_avg_amplitude[0]}, Max Probability - {max_probabilities_up_avg_amplitude[0]:.4f}\")\n",
    "print(f\"3rd Model: Predicted Class - {predicted_labels_up_epoch_test[0]}, Max Probability - {np.max(predicted_probabilities_up_epoch_test[0]):.8f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Prediction Label: 1\n"
     ]
    }
   ],
   "source": [
    "def combined_prediction(acc1, acc2, acc3, prob1, prob2, prob3, class1, class2, class3):\n",
    "    weighted_avg = (acc1 * prob1 * class1 + acc2 * prob2 * class2 + acc3 * prob3 * class3) / (acc1 + acc2 + acc3)\n",
    "    return weighted_avg\n",
    "acc1 = 0.3095  # Accuracy of model 1\n",
    "acc2 = 0.4762  # Accuracy of model 2\n",
    "acc3 = 0.5476  # Accuracy of model 3\n",
    "\n",
    "prob1 = max_probabilities1[0]  \n",
    "prob2 = max_probabilities_up_avg_amplitude[0]  \n",
    "prob3 = np.max(predicted_probabilities_up_epoch_test[0])  \n",
    "\n",
    "class1 = predicted_labels1[0] \n",
    "class2 = max_class_indices_up_avg_amplitude[0]  \n",
    "class3 = predicted_labels_up_epoch_test[0]  \n",
    "\n",
    "combined_pred = combined_prediction(acc1, acc2, acc3, prob1, prob2, prob3, class1, class2, class3)\n",
    "combined_pred_label = round(combined_pred)  \n",
    "print(f\"Combined Prediction Label: {combined_pred_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
